{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from cvnn import layers\n",
    "from pdb import set_trace\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "INPUT_SIZE = (572, 572)\n",
    "MASK_SIZE = (388, 388)\n",
    "\n",
    "\n",
    "def _downsample_tf(inputs, units):\n",
    "    c0 = tf.keras.layers.Conv2D(units, activation='relu', kernel_size=3)(inputs)\n",
    "    c1 = tf.keras.layers.Conv2D(units, activation='relu', kernel_size=3)(c0)\n",
    "    c2 = tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(c1)\n",
    "    return c0, c1, c2\n",
    "\n",
    "\n",
    "def _downsample_cvnn(inputs, units, dtype=tf.float32):\n",
    "    c0 = layers.ComplexConv2D(units, activation='cart_relu', kernel_size=3, dtype=dtype)(inputs)\n",
    "    c1 = layers.ComplexConv2D(units, activation='cart_relu', kernel_size=3, dtype=dtype)(c0)\n",
    "    c2 = layers.ComplexMaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid', dtype=dtype)(c1)\n",
    "    return c0, c1, c2\n",
    "\n",
    "\n",
    "def _upsample_tf(in1, in2, units, crop):\n",
    "    t01 = tf.keras.layers.Conv2DTranspose(units, kernel_size=2, strides=(2, 2), activation='relu')(in1)\n",
    "    crop01 = tf.keras.layers.Cropping2D(cropping=(crop, crop))(in2)\n",
    "\n",
    "    concat01 = tf.keras.layers.concatenate([t01, crop01], axis=-1)\n",
    "\n",
    "    out1 = tf.keras.layers.Conv2D(units, activation='relu', kernel_size=3)(concat01)\n",
    "    out2 = tf.keras.layers.Conv2D(units, activation='relu', kernel_size=3)(out1)\n",
    "    return out1, out2\n",
    "\n",
    "\n",
    "def _upsample_cvnn(in1, in2, units, crop, dtype=tf.float32):\n",
    "    t01 = layers.ComplexConv2DTranspose(units, kernel_size=2, strides=(2, 2), activation='relu', dtype=dtype)(in1)\n",
    "    crop01 = tf.keras.layers.Cropping2D(cropping=(crop, crop))(in2)\n",
    "\n",
    "    concat01 = tf.keras.layers.concatenate([t01, crop01], axis=-1)\n",
    "\n",
    "    out1 = layers.ComplexConv2D(units, activation='relu', kernel_size=3, dtype=dtype)(concat01)\n",
    "    out2 = layers.ComplexConv2D(units, activation='relu', kernel_size=3, dtype=dtype)(out1)\n",
    "    return out1, out2\n",
    "\n",
    "\n",
    "def normalize(input_image, input_mask):\n",
    "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "    # input_mask -= 1\n",
    "    return input_image, input_mask\n",
    "\n",
    "\n",
    "def load_image(datapoint):\n",
    "    input_image = tf.image.resize_with_pad(datapoint['image'], INPUT_SIZE[0], INPUT_SIZE[1])\n",
    "    input_mask = tf.image.resize_with_pad(datapoint['segmentation_mask'], MASK_SIZE[0], MASK_SIZE[1])\n",
    "\n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "\n",
    "    return input_image, input_mask\n",
    "\n",
    "\n",
    "def get_dataset():\n",
    "    (train_images, test_images), info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True,\n",
    "                                                  split=['train[:1%]', 'test[:1%]'])\n",
    "    train_length = info.splits['train'].num_examples\n",
    "    steps_per_epoch = train_length // BATCH_SIZE\n",
    "    train_images = train_images.map(load_image)\n",
    "    test_images = test_images.map(load_image)\n",
    "\n",
    "    train_batches = train_images.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    test_batches = test_images.batch(BATCH_SIZE)\n",
    "    # set_trace()\n",
    "    return train_batches, test_batches\n",
    "\n",
    "\n",
    "def get_cvnn_model(dtype=tf.float32):\n",
    "    tf.random.set_seed(1)\n",
    "    inputs = layers.complex_input(shape=INPUT_SIZE + (3,), dtype=dtype)\n",
    "    # inputs = tf.keras.layers.InputLayer(input_shape=INPUT_SIZE + (3,), dtype=dtype)\n",
    "    # inputs = tf.keras.layers.Input(shape=INPUT_SIZE + (3,))\n",
    "\n",
    "    c0, c1, c2 = _downsample_cvnn(inputs, 64, dtype)\n",
    "    c3, c4, c5 = _downsample_cvnn(c2, 128, dtype)\n",
    "    c6, c7, c8 = _downsample_cvnn(c5, 256, dtype)\n",
    "    c9, c10, c11 = _downsample_cvnn(c8, 512, dtype)\n",
    "\n",
    "    c12 = layers.ComplexConv2D(1024, activation='relu', kernel_size=3, dtype=dtype)(c11)\n",
    "    c13 = layers.ComplexConv2D(1024, activation='relu', kernel_size=3, padding='valid', dtype=dtype)(c12)\n",
    "\n",
    "    c14, c15 = _upsample_cvnn(c13, c10, 512, 4, dtype)\n",
    "    c16, c17 = _upsample_cvnn(c15, c7, 256, 16, dtype)\n",
    "    c18, c19 = _upsample_cvnn(c17, c4, 128, 40, dtype)\n",
    "    c20, c21 = _upsample_cvnn(c19, c1, 64, 88, dtype)\n",
    "\n",
    "    outputs = layers.ComplexConv2D(4, kernel_size=1, dtype=dtype)(c21)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"u-net-cvnn\")\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_tf_model():\n",
    "    tf.random.set_seed(1)\n",
    "    inputs = tf.keras.layers.Input(shape=INPUT_SIZE + (3,))\n",
    "\n",
    "    c0, c1, c2 = _downsample_tf(inputs, 64)\n",
    "    c3, c4, c5 = _downsample_tf(c2, 128)\n",
    "    c6, c7, c8 = _downsample_tf(c5, 256)\n",
    "    c9, c10, c11 = _downsample_tf(c8, 512)\n",
    "\n",
    "    c12 = tf.keras.layers.Conv2D(1024, activation='relu', kernel_size=3)(c11)\n",
    "    c13 = tf.keras.layers.Conv2D(1024, activation='relu', kernel_size=3, padding='valid')(c12)\n",
    "\n",
    "    c14, c15 = _upsample_tf(c13, c10, 512, 4)\n",
    "    c16, c17 = _upsample_tf(c15, c7, 256, 16)\n",
    "    c18, c19 = _upsample_tf(c17, c4, 128, 40)\n",
    "    c20, c21 = _upsample_tf(c19, c1, 64, 88)\n",
    "\n",
    "    outputs = tf.keras.layers.Conv2D(4, kernel_size=1)(c21)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"u-net-tf\")\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_model(model, train_batches, test_batches):\n",
    "    weigths = model.get_weights()\n",
    "    # with tf.GradientTape() as tape:\n",
    "    #     # for elem, label in iter(ds_train):\n",
    "    #     loss = model.compiled_loss(y_true=tf.convert_to_tensor(test_labels), y_pred=model(test_images))\n",
    "    #     gradients = tape.gradient(loss, model.trainable_weights)  # back-propagation\n",
    "    logs = {\n",
    "        'weights': weigths,\n",
    "        # 'loss': loss,\n",
    "        # 'gradients': gradients\n",
    "    }\n",
    "\n",
    "    history = model.fit(train_batches, epochs=2, validation_data=test_batches)\n",
    "    return history, logs\n",
    "\n",
    "\n",
    "def test_unet():\n",
    "    train_batches, test_batches = get_dataset()\n",
    "    history_own, logs_own = test_model(get_cvnn_model(), train_batches, test_batches)\n",
    "    history_keras, logs_keras = test_model(get_tf_model(), train_batches, test_batches)\n",
    "    assert history_keras.history == history_own.history, f\"\\n{history_keras.history}\\n !=\\n{history_own.history}\"\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     from importlib import reload\n",
    "#     import os\n",
    "#     import tensorflow\n",
    "\n",
    "#     reload(tensorflow)\n",
    "#     os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "#     test_unet()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
